{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline , FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///disaster.db')\n",
    "conn = engine.connect()\n",
    "df = pd.read_sql_table('disaster_messages', conn)\n",
    "#df = pd.DataFrame(engine.connect().execute(text('SELECT * FROM disaster_messages')))\n",
    "X = df['message']\n",
    "Y = df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # iterate through each token\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        \n",
    "        # lemmatize, normalize case, and remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    \n",
    "    #Remove stop words to reduce training time\n",
    "    #In fact this improves sensitivity of the model as i found out in the process\n",
    "    clean_tokens = [w for w in clean_tokens if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize, token_pattern=None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "# train pipeline\n",
    "pipeline.fit(X_train, Y_train)\n",
    "# predict on test data\n",
    "Y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_mod = pd.DataFrame(Y_pred, columns = Y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.32      0.44      1531\n",
      "           1       0.82      0.96      0.88      4976\n",
      "           2       1.00      0.09      0.16        47\n",
      "\n",
      "    accuracy                           0.81      6554\n",
      "   macro avg       0.85      0.46      0.50      6554\n",
      "weighted avg       0.80      0.81      0.78      6554\n",
      "\n",
      "request\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      5419\n",
      "           1       0.87      0.48      0.62      1135\n",
      "\n",
      "    accuracy                           0.90      6554\n",
      "   macro avg       0.88      0.73      0.78      6554\n",
      "weighted avg       0.89      0.90      0.88      6554\n",
      "\n",
      "offer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6522\n",
      "           1       0.00      0.00      0.00        32\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      1.00      0.99      6554\n",
      "\n",
      "aid_related\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82      3848\n",
      "           1       0.77      0.68      0.72      2706\n",
      "\n",
      "    accuracy                           0.78      6554\n",
      "   macro avg       0.78      0.77      0.77      6554\n",
      "weighted avg       0.78      0.78      0.78      6554\n",
      "\n",
      "medical_help\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      6051\n",
      "           1       0.67      0.06      0.11       503\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.80      0.53      0.54      6554\n",
      "weighted avg       0.91      0.93      0.90      6554\n",
      "\n",
      "medical_products\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      6230\n",
      "           1       0.79      0.07      0.12       324\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.87      0.53      0.55      6554\n",
      "weighted avg       0.95      0.95      0.93      6554\n",
      "\n",
      "search_and_rescue\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      6365\n",
      "           1       0.68      0.07      0.12       189\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.83      0.53      0.56      6554\n",
      "weighted avg       0.96      0.97      0.96      6554\n",
      "\n",
      "security\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6440\n",
      "           1       0.00      0.00      0.00       114\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "military\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      6357\n",
      "           1       0.73      0.06      0.10       197\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.85      0.53      0.54      6554\n",
      "weighted avg       0.96      0.97      0.96      6554\n",
      "\n",
      "child_alone\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6554\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       1.00      1.00      1.00      6554\n",
      "weighted avg       1.00      1.00      1.00      6554\n",
      "\n",
      "water\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6114\n",
      "           1       0.91      0.35      0.50       440\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.93      0.67      0.74      6554\n",
      "weighted avg       0.95      0.95      0.94      6554\n",
      "\n",
      "food\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      5782\n",
      "           1       0.87      0.52      0.65       772\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.91      0.76      0.81      6554\n",
      "weighted avg       0.93      0.93      0.93      6554\n",
      "\n",
      "shelter\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      5964\n",
      "           1       0.83      0.39      0.53       590\n",
      "\n",
      "    accuracy                           0.94      6554\n",
      "   macro avg       0.89      0.69      0.75      6554\n",
      "weighted avg       0.93      0.94      0.93      6554\n",
      "\n",
      "clothing\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6445\n",
      "           1       0.78      0.06      0.12       109\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.88      0.53      0.56      6554\n",
      "weighted avg       0.98      0.98      0.98      6554\n",
      "\n",
      "money\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6399\n",
      "           1       1.00      0.03      0.05       155\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.99      0.51      0.52      6554\n",
      "weighted avg       0.98      0.98      0.97      6554\n",
      "\n",
      "missing_people\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6466\n",
      "           1       1.00      0.03      0.07        88\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.99      0.52      0.53      6554\n",
      "weighted avg       0.99      0.99      0.98      6554\n",
      "\n",
      "refugees\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      6325\n",
      "           1       0.50      0.01      0.02       229\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.73      0.50      0.50      6554\n",
      "weighted avg       0.95      0.97      0.95      6554\n",
      "\n",
      "death\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6260\n",
      "           1       0.83      0.12      0.21       294\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.90      0.56      0.59      6554\n",
      "weighted avg       0.95      0.96      0.94      6554\n",
      "\n",
      "other_aid\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93      5735\n",
      "           1       0.70      0.03      0.05       819\n",
      "\n",
      "    accuracy                           0.88      6554\n",
      "   macro avg       0.79      0.51      0.49      6554\n",
      "weighted avg       0.86      0.88      0.82      6554\n",
      "\n",
      "infrastructure_related\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      6130\n",
      "           1       0.50      0.00      0.00       424\n",
      "\n",
      "    accuracy                           0.94      6554\n",
      "   macro avg       0.72      0.50      0.49      6554\n",
      "weighted avg       0.91      0.94      0.90      6554\n",
      "\n",
      "transport\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      6223\n",
      "           1       0.73      0.07      0.13       331\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.84      0.54      0.55      6554\n",
      "weighted avg       0.94      0.95      0.93      6554\n",
      "\n",
      "buildings\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      6221\n",
      "           1       0.83      0.11      0.20       333\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.89      0.56      0.59      6554\n",
      "weighted avg       0.95      0.95      0.94      6554\n",
      "\n",
      "electricity\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6422\n",
      "           1       0.57      0.03      0.06       132\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.78      0.51      0.52      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "tools\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6510\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      0.99      0.99      6554\n",
      "\n",
      "hospitals\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6481\n",
      "           1       0.00      0.00      0.00        73\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "shops\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6521\n",
      "           1       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      0.99      0.99      6554\n",
      "\n",
      "aid_centers\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6473\n",
      "           1       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "other_infrastructure\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6276\n",
      "           1       0.00      0.00      0.00       278\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.48      0.50      0.49      6554\n",
      "weighted avg       0.92      0.96      0.94      6554\n",
      "\n",
      "weather_related\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92      4702\n",
      "           1       0.85      0.68      0.76      1852\n",
      "\n",
      "    accuracy                           0.88      6554\n",
      "   macro avg       0.87      0.82      0.84      6554\n",
      "weighted avg       0.87      0.88      0.87      6554\n",
      "\n",
      "floods\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      5991\n",
      "           1       0.92      0.44      0.59       563\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.93      0.72      0.78      6554\n",
      "weighted avg       0.95      0.95      0.94      6554\n",
      "\n",
      "storm\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      5937\n",
      "           1       0.78      0.50      0.61       617\n",
      "\n",
      "    accuracy                           0.94      6554\n",
      "   macro avg       0.86      0.74      0.79      6554\n",
      "weighted avg       0.93      0.94      0.93      6554\n",
      "\n",
      "fire\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6484\n",
      "           1       0.00      0.00      0.00        70\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "earthquake\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      5950\n",
      "           1       0.88      0.77      0.82       604\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.93      0.88      0.90      6554\n",
      "weighted avg       0.97      0.97      0.97      6554\n",
      "\n",
      "cold\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6406\n",
      "           1       0.78      0.05      0.09       148\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.88      0.52      0.54      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "other_weather\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      6204\n",
      "           1       0.64      0.05      0.09       350\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.79      0.52      0.53      6554\n",
      "weighted avg       0.93      0.95      0.93      6554\n",
      "\n",
      "direct_report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      5282\n",
      "           1       0.82      0.37      0.51      1272\n",
      "\n",
      "    accuracy                           0.86      6554\n",
      "   macro avg       0.85      0.67      0.71      6554\n",
      "weighted avg       0.86      0.86      0.84      6554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in Y_pred_mod.columns:\n",
    "    print(col)\n",
    "    print()\n",
    "    print(classification_report(Y_test[col], Y_pred_mod[col], zero_division=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect', CountVectorizer(token_pattern=None,\n",
       "                   tokenizer=<function tokenize at 0x164405c60>)),\n",
       "  ('tfidf', TfidfTransformer()),\n",
       "  ('clf', MultiOutputClassifier(estimator=RandomForestClassifier()))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(token_pattern=None,\n",
       "                 tokenizer=<function tokenize at 0x164405c60>),\n",
       " 'tfidf': TfidfTransformer(),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier()),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': None,\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'sqrt',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__monotonic_cst': None,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters for grid search\n",
    "param_grid = {\n",
    "    'clf__estimator__n_estimators': [100, 200, 300],\n",
    "    'clf__estimator__max_depth': [None, 5, 10],\n",
    "    'clf__estimator__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "cv = GridSearchCV(pipeline, param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to only modify one parameter here because  it took a long time to calculate for more parameters. Also i believe to have realised a problem with GridSearchCV in combination with MultiOutputClassifier: GridSearchCV needs to evaluate the paramater combination by certain scorings. The optimal scoring to decide on is relatively difficult  to find because in MultiOutput you have different scores for each category. Also, i don't know yet which of the 36 scores exactly GridSearchCV takes to evaluate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(X_train, Y_train)\n",
    "Y_pred_cv = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try other parameters and RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__norm' : ['l1', 'l2'],\n",
    "    'tfidf__smooth_idf': [True, False],\n",
    "    'tfidf__sublinear_tf': [True, False],\n",
    "    'tfidf__use_idf': [True, False],\n",
    "    'clf__estimator__max_depth': [3, 5, 10],\n",
    "    'clf__estimator__n_estimators': [50, 100, 500],\n",
    "    'clf__estimator__verbose': [0, 5, 20]\n",
    "}\n",
    "\n",
    "cv1 = RandomizedSearchCV(pipeline, param_distributions=parameters, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1.fit(X_train, Y_train)\n",
    "Y_pred_cv1 = cv1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_cv_mod = pd.DataFrame(Y_pred_cv)\n",
    "Y_pred_cv_mod.columns = Y_test.columns\n",
    "\n",
    "for col in Y_pred_cv_mod.columns:\n",
    "    print(col)\n",
    "    print()\n",
    "    print(classification_report(Y_test[col], Y_pred_cv_mod[col], zero_division=0.0))\n",
    "    \n",
    "print(\"\\nBest Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_cv1_mod = pd.DataFrame(Y_pred_cv1)\n",
    "Y_pred_cv1_mod.columns = Y_test.columns\n",
    "\n",
    "for col in Y_pred_cv1_mod.columns:\n",
    "    print(col)\n",
    "    print()\n",
    "    print(classification_report(Y_test[col], Y_pred_cv1_mod[col], zero_division=0.0))\n",
    "    \n",
    "print(\"\\nBest Parameters:\", cv1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: GridSearchCV and RandomizedSearchCV did not lead to better results. In fact, sensitivity of the model was totally killed. Both searches lead to only zeros. It seems i did not choose the best combinations of Hyperparameters to dive into. Also maybe the scoring parameters lead the model to choosing not really the best parameter. For now it's too much for me to handle alone. If to dive deep into readings, which i will, to improve parameter settings like scoring and hyperparameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to try and include the clustering feature KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "pipeline_new = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize, token_pattern=None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('kmeans', KMeans()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pipeline\n",
    "pipeline_new.fit(X_train, Y_train)\n",
    "# predict on test data\n",
    "Y_pred = pipeline_new.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_mod = pd.DataFrame(Y_pred, columns=Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in Y_pred_mod.columns:\n",
    "    print(col)\n",
    "    print()\n",
    "    print(classification_report(Y_test[col], Y_pred_mod[col], zero_division=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are somewhat worse then the original results. Maybe implementing the feature one after the other did weaken the effects of tfidf. Let's try a FeatureUnion of tfidf and KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_new = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize, token_pattern=None)),\n",
    "    ('feature_union', FeatureUnion([\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('kmeans', KMeans()),\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pipeline\n",
    "pipeline_new.fit(X_train, Y_train)\n",
    "# predict on test data\n",
    "Y_pred = pipeline_new.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_mod = pd.DataFrame(Y_pred, columns=Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in Y_pred_mod.columns:\n",
    "    print(col)\n",
    "    print()\n",
    "    print(classification_report(Y_test[col], Y_pred_mod[col], zero_division=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not a success. The precision and recall values are in fact a little lower than in the original results.\n",
    "Let's export our original 'pipeline' then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(pipeline, open('disaster-response-model.pk1' , 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
